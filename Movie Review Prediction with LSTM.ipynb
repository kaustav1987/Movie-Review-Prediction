{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Prediction with LSTM\n",
    "\n",
    "In this notebook, we'll implement a recurrent neural network that performs movie review prediction. Here we'll use a dataset of movie reviews, accompanied by sentiment labels: positive or negative.\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        What a wonderful movie!! üëç (positive)\n",
    "\n",
    "        I didn't like the main charecter üëé (negetive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the file\n",
    "\n",
    "with open('data/reviews.txt') as f:\n",
    "    reviews = f.read()\n",
    "    \n",
    "with open('data/labels.txt') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negat\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:1000])\n",
    "print(labels[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get rid of punctuations\n",
    "reviews = reviews.lower()\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_split = all_text.split('\\n')\n",
    "reviews_split_len = np.zeros((len(reviews_split)))\n",
    "for i,review in enumerate(reviews_split):\n",
    "    reviews_split_len[i] = len(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13740.0\n"
     ]
    }
   ],
   "source": [
    "## Average length of each review is 1332. But this may be too long to process\n",
    "\n",
    "np.mean(reviews_split_len)\n",
    "print(max(reviews_split_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Review Length Distribution')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(reviews_split_len,bins = 30)\n",
    "plt.grid()\n",
    "plt.xlabel('len of reviews')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Review Length Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(least_used)\n",
    "#len(words_count)\n",
    "\n",
    "##There are 50k words which are used less than or equal to 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50154\n"
     ]
    }
   ],
   "source": [
    "##text_words = text\n",
    "\n",
    "## The top 19 are the words are the words which are words which are used in most of the reviews\n",
    "## irrespective of the \n",
    "words_count = Counter(all_text.split(' '))\n",
    "most_common_19 = words_count.most_common(19)\n",
    "\n",
    "top19words = []\n",
    "for (word,count) in most_common_19:\n",
    "    top19words.append(word)\n",
    "    \n",
    "top19words.remove('')\n",
    "top19words\n",
    "\n",
    "##**********\n",
    "\n",
    "## Now lets check the least used words....\n",
    "\n",
    "least_used = []\n",
    "for (word,count) in words_count.items():\n",
    "    if count <= 5:\n",
    "        least_used.append(word)\n",
    "print(len(least_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "##result = all_text.split(' ')\n",
    "#for item in top19words:\n",
    "    #raw_item = re.escape(item)\n",
    "    ##raw_item = r\n",
    "#result = re.sub(r'the',  r'',    result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define the dictionary\n",
    "def dic(all_text):\n",
    "    reviews_split= all_text.split('\\n')\n",
    "    all_text = ' '.join(reviews_split)\n",
    "    words = all_text.split()\n",
    "    word_count = Counter(words)\n",
    "    vocab = sorted(word_count, key = word_count.get, reverse = True)\n",
    "    vocab_2_int = {word:ii for ii, word in enumerate(vocab,1)}\n",
    "    int_2_vocab = { ii:word for word,ii in vocab_2_int.items()}\n",
    "    tokens = str(set(words))\n",
    "    return reviews_split,tokens, vocab_2_int,int_2_vocab\n",
    "\n",
    "###\n",
    "reviews_split,tokens, vocab_2_int,int_2_vocab = dic(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review2int(reviews_split,vocab_2_int):\n",
    "    reviews_int= []\n",
    "    for review in reviews_split:\n",
    "        reviews_int.append([vocab_2_int[word] for word in review.split() if vocab_2_int.get(word,-1) != -1])\n",
    "    return reviews_int\n",
    "    \n",
    "### Test\n",
    "\n",
    "reviews_int = review2int(reviews_split,vocab_2_int)\n",
    "len(reviews_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in reviews_int])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert target to numeric\n",
    "def label2int(labels):\n",
    "    labels = labels.split('\\n')\n",
    "    targets = []\n",
    "    for label in labels:\n",
    "        if label == 'positive':\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "        \n",
    "    encoded_labels =np.array(targets)\n",
    "    return encoded_labels\n",
    "\n",
    "## test\n",
    "\n",
    "encoded_labels = label2int(labels)\n",
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove NULL reviews\n",
    "\n",
    "def remove_NULL_review(reviews_int,encoded_labels):\n",
    "    reviews_int_upd = []\n",
    "    encoded_labels_upd = []\n",
    "    for i,review in enumerate(reviews_int):\n",
    "        if len(review) != 0:\n",
    "            reviews_int_upd.append(reviews_int[i])\n",
    "            encoded_labels_upd.append(encoded_labels[i])\n",
    "    reviews_int = np.array(reviews_int_upd)\n",
    "    encoded_labels = np.array(encoded_labels_upd)\n",
    "    return reviews_int,encoded_labels\n",
    "\n",
    "##Test\n",
    "reviews_int,encoded_labels = remove_NULL_review(reviews_int,encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our dictionary vocab doesn't have the value 0. So we will use 0 to pad all the reviews of uniform length\n",
    "\n",
    "def padd_features(reviews_int,seq_length):\n",
    "    rows = len(reviews_int)\n",
    "    features = np.zeros((rows, seq_length),dtype = int)\n",
    "    for i in range(rows):\n",
    "        each_review_len = len(reviews_int[i])\n",
    "        if each_review_len < seq_length:\n",
    "            start_pos = seq_length - each_review_len\n",
    "            features[i,start_pos:] = reviews_int[i]\n",
    "        else:\n",
    "            features[i,:] = reviews_int[i][:seq_length]\n",
    "    return features\n",
    "\n",
    "## test \n",
    "seq_length =200\n",
    "features = padd_features(reviews_int,seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   54    10    14   116    60   798   552    71   364     5]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   330   578    34     3   162   748  2731     9   325]\n",
      " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
      " [    1    20     6    76    40     6    58    81    95     5]\n",
      " [   54    10    84   329 26230 46427    63    10    14   614]\n",
      " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   40    26   109 17952  1422     9     1   327     4   125]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   10   499     1   307 10399    55    74     8    13    30]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "## Veify all the features are of Seq_length  now\n",
    "\n",
    "count = 0\n",
    "for i in range(len(features)):\n",
    "    if len(features[i]) != 200:\n",
    "        print(i)\n",
    "    else:\n",
    "        count += 1\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build Dataloader\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "train_ratio = 0.8\n",
    "batch_size =32\n",
    "\n",
    "def batch_data(features, encoded_labels,batch_size, train_ratio):\n",
    "    \n",
    "    #n_batches = len(features)//batch_size  ### Seq length not not required here .SeqLength will be used for padding\n",
    "    #total_full_batches= n_batches*batch_size\n",
    "    #features = features [:total_full_batches]\n",
    "    #encoded_labels = encoded_labels[:total_full_batches]\n",
    "    \n",
    "    assert(features.shape[0] == encoded_labels.shape[0])\n",
    "    \n",
    "    train_idx = int(len(features)*0.8)\n",
    "    train_x , train_y= features[:train_idx], encoded_labels[:train_idx]\n",
    "    \n",
    "    valid_idx = train_idx + (len(features)- train_idx)//2\n",
    "    valid_x, valid_y = features[train_idx:valid_idx], encoded_labels[train_idx:valid_idx]\n",
    "    \n",
    "    test_x , test_y = features[valid_idx:], encoded_labels[valid_idx:]\n",
    "    \n",
    "    train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "    \n",
    "    train_loader = DataLoader(train_data, shuffle = True , batch_size = batch_size,drop_last=True)\n",
    "    valid_loader = DataLoader(valid_data, shuffle = True , batch_size = batch_size,drop_last=True)\n",
    "    test_loader =  DataLoader(test_data,  shuffle = True , batch_size = batch_size,drop_last=True)\n",
    "    \n",
    "    return train_loader,valid_loader,test_loader\n",
    "\n",
    "##test \n",
    "\n",
    "train_loader,valid_loader,test_loader = batch_data(features, encoded_labels,batch_size, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "## Test the Dataloader\n",
    "\n",
    "dataiter =  iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('NOT Training on GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,input_size, output_size,hidden_dim, embedding_dim,n_layers, drop_prob):\n",
    "        \n",
    "        super(SentimentRNN,self).__init__()\n",
    "        self.vocab_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size,embedding_dim)\n",
    "        self.lstm      = nn.LSTM(embedding_dim,hidden_dim,n_layers, dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        ##batch_size = x.shape[0]\n",
    "        batch_size = x.size(0)\n",
    "        if train_on_gpu: \n",
    "            x = x.type(torch.cuda.LongTensor) \n",
    "        else: \n",
    "            x = x_input.type(torch.LongTensor)\n",
    "        \n",
    "        embed_out = self.embedding(x)\n",
    "        lstm_out, h = self.lstm(embed_out,hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        r_out = self.dropout(lstm_out)\n",
    "        out = self.fc(r_out)\n",
    "        out= self.sig(out)\n",
    "        out = out.view(batch_size,-1)\n",
    "        \n",
    "        out = out[:,-1] ## Take only the last column as output\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "          \n",
    "        weight = next(self.parameters()).data\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers,batch_size,self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers,batch_size,self.hidden_dim).zero_())\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74072"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_2_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(74073, 400)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hyper Parameters\n",
    "\n",
    "train_ratio = 0.8\n",
    "lr = .0005\n",
    "input_size = len(vocab_2_int) + 1 ## extra 1 for zero padding \n",
    "output_size = 1\n",
    "n_layers = 2\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_epochs = 4\n",
    "drop_prob= 0.3\n",
    "batch_size = 50\n",
    "clip = 5\n",
    "print_every = 100\n",
    "\n",
    "#####***********\n",
    "vocab_size = input_size\n",
    "epochs = 5\n",
    "#####***********\n",
    "\n",
    "## Create the train,validation and test set\n",
    "features = padd_features(reviews_int,seq_length)\n",
    "train_loader,valid_loader,test_loader = batch_data(features, encoded_labels,batch_size, train_ratio)\n",
    "\n",
    "## Creating the instance of Class\n",
    "\n",
    "#RNN = SentimentRNN(input_size, output_size,hidden_dim, embedding_dim,n_layers, drop_prob)\n",
    "\n",
    "#if train_on_gpu:\n",
    "#    RNN = RNN.cuda()\n",
    "\n",
    "\n",
    "#criterion = nn.BCELoss()\n",
    "#optimizer = torch.optim.Adam(RNN.parameters(), lr =lr)\n",
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr,n_epochs,batch_size, print_every, train_loader,valid_loader):\n",
    "    \n",
    "    model_name = 'sentiment_rnn.pt'\n",
    "    validation_loss = np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        h = RNN.init_hidden(batch_size)\n",
    "        counter = 0\n",
    "        \n",
    "        ###Train Loop\n",
    "        for inputs,labels in train_loader:\n",
    "\n",
    "            RNN.train()\n",
    "            input_losses = []\n",
    "            counter +=1\n",
    "            ##print(inputs.shape)\n",
    "            h = tuple([each.data for each in h])\n",
    "            if train_on_gpu:\n",
    "                labels = labels.type(torch.cuda.FloatTensor)\n",
    "            else:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                \n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            output,h  = RNN(inputs,h)\n",
    "            \n",
    "            RNN.zero_grad()\n",
    "            \n",
    "            loss = criterion(output,labels)\n",
    "            loss.backward()\n",
    "            input_losses.append(loss.item())\n",
    "            \n",
    "            ## cliping required ###\n",
    "            nn.utils.clip_grad_norm_(RNN.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            if counter% print_every == 0:\n",
    "                ##  Execute the function for validation set \n",
    "                val_h = RNN.init_hidden(batch_size)\n",
    "                validation_losses = []\n",
    "                RNN.eval()\n",
    "                for inputs,labels in valid_loader:\n",
    "                    if train_on_gpu:\n",
    "                        inputs,labels = inputs.cuda(), labels.cuda()\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    if train_on_gpu:\n",
    "                        labels = labels.type(torch.cuda.FloatTensor)\n",
    "                    else:\n",
    "                        labels = labels.type(torch.LongTensor)\n",
    "                        \n",
    "                    output,val_h = RNN(inputs,val_h)\n",
    "                    loss = criterion(output,labels)\n",
    "                    validation_losses.append(loss.item())\n",
    "                print('Epoch {}/{} '.format(epoch+1,n_epochs),\n",
    "                      'Step {}'.format(counter),\n",
    "                      'Train Loss {:.6f}'.format(loss.item()),\n",
    "                       'Valid Loss {:.6f}'.format(np.average(validation_losses)))\n",
    "                if validation_loss > np.average(validation_losses):\n",
    "                    print('Saving Model')\n",
    "                    validation_loss = np.average(validation_losses)\n",
    "                    ## Function call to save the model\n",
    "                    checkpoint = {'input_size': input_size,\n",
    "                                  'output_size': output_size,\n",
    "                                  'hidden_dim': hidden_dim,\n",
    "                                  'embedding_dim':embedding_dim,\n",
    "                                  'n_layers':n_layers,\n",
    "                                   'drop_prob': drop_prob,\n",
    "                                   'state_dict': RNN.state_dict()}\n",
    "                    with open(model_name, 'wb') as f:\n",
    "                        torch.save(checkpoint,f)\n",
    "                                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(74073, 400)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = 'N'\n",
    "\n",
    "## LOading the model\n",
    "\n",
    "if load_model == 'Y':\n",
    "    with open('sentiment_rnn.pt', 'rb') as f:\n",
    "        checkpoint = torch.load(f)\n",
    "    ##model = CharRNN(checkpoint['tokens'],n_hidden =checkpoint['n_hidden'],n_layers=checkpoint['n_layers'])\n",
    "    RNN = RNN(checkpoint['vocab_size'], checkpoint['output_size'], \n",
    "              checkpoint['embedding_dim'], checkpoint['hidden_dim'], checkpoint['n_layers'])\n",
    "    RNN.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    RNN = SentimentRNN(input_size, output_size,hidden_dim, embedding_dim,n_layers, drop_prob)\n",
    "    \n",
    "if train_on_gpu:\n",
    "    RNN = RNN.cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(RNN.parameters(), lr =lr)\n",
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4  Step 100 Train Loss 0.573483 Valid Loss 0.621249\n",
      "Saving Model\n",
      "Epoch 1/4  Step 200 Train Loss 0.618814 Valid Loss 0.646554\n",
      "Epoch 1/4  Step 300 Train Loss 0.696008 Valid Loss 0.693203\n",
      "Epoch 1/4  Step 400 Train Loss 0.691046 Valid Loss 0.691830\n",
      "Epoch 2/4  Step 100 Train Loss 0.602235 Valid Loss 0.592684\n",
      "Saving Model\n",
      "Epoch 2/4  Step 200 Train Loss 0.589072 Valid Loss 0.587632\n",
      "Saving Model\n",
      "Epoch 2/4  Step 300 Train Loss 0.597116 Valid Loss 0.596245\n",
      "Epoch 2/4  Step 400 Train Loss 0.620176 Valid Loss 0.498847\n",
      "Saving Model\n",
      "Epoch 3/4  Step 100 Train Loss 0.527976 Valid Loss 0.518637\n",
      "Epoch 3/4  Step 200 Train Loss 0.419137 Valid Loss 0.502183\n",
      "Epoch 3/4  Step 300 Train Loss 0.494283 Valid Loss 0.476994\n",
      "Saving Model\n",
      "Epoch 3/4  Step 400 Train Loss 0.398405 Valid Loss 0.473083\n",
      "Saving Model\n",
      "Epoch 4/4  Step 100 Train Loss 0.451135 Valid Loss 0.512223\n",
      "Epoch 4/4  Step 200 Train Loss 0.608888 Valid Loss 0.457768\n",
      "Saving Model\n",
      "Epoch 4/4  Step 300 Train Loss 0.606621 Valid Loss 0.443523\n",
      "Saving Model\n",
      "Epoch 4/4  Step 400 Train Loss 0.417979 Valid Loss 0.462270\n"
     ]
    }
   ],
   "source": [
    "train(lr,n_epochs,batch_size, print_every, train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading the model\n",
    "\n",
    "with open('sentiment_rnn.pt', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    ##model = CharRNN(checkpoint['tokens'],n_hidden =checkpoint['n_hidden'],n_layers=checkpoint['n_layers'])\n",
    "RNN = SentimentRNN(checkpoint['input_size'], checkpoint['output_size'], \n",
    "              checkpoint['hidden_dim'],checkpoint['embedding_dim'], checkpoint['n_layers'],checkpoint['drop_prob'] )\n",
    "RNN.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "RNN = RNN.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 0.446404 Test Accuracy 79.08 %\n"
     ]
    }
   ],
   "source": [
    "## Verify the test set\n",
    "\n",
    "num_correct = 0\n",
    "test_losses = []\n",
    "test_h = RNN.init_hidden(batch_size)\n",
    "\n",
    "RNN.eval()\n",
    "for inputs,labels in test_loader:\n",
    "    if train_on_gpu:\n",
    "        inputs,labels = inputs.cuda(), labels.cuda()\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    if train_on_gpu:\n",
    "        labels = labels.type(torch.cuda.FloatTensor)\n",
    "    else:\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "                        \n",
    "    output,test_h = RNN(inputs,test_h)\n",
    "    loss = criterion(output,labels)\n",
    "    test_losses.append(loss.item())\n",
    "    pred = torch.round(output.squeeze())\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    if train_on_gpu:\n",
    "        correct = correct_tensor.cpu().numpy()\n",
    "    else:\n",
    "        correct = correct_tensor.numpy()\n",
    "    num_correct +=np.sum(correct)\n",
    "\n",
    "## Test loss and test accuracy\n",
    "print('Test Loss {:.6f}'.format(np.mean(test_losses)),\n",
    "      'Test Accuracy {:.2f} %'.format((num_correct*100/len(test_loader.dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference on a test review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
